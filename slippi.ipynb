{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "slippi.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyP2qGzlwSMXsgnoT3DBks1x",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CameronKenworthyCode/python/blob/main/slippi.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iFkVNrQZKgtY"
      },
      "source": [
        "I'm always working with the large (>5GB) data on a Gdrive, so first things first, mount the drive, so this assumes you have a G-drive with replay files in a folder, I provide a link to the post-wrangled csv file later. But this is part of the prep work for the project"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xtgx1Cj1KcRl"
      },
      "source": [
        "!pip install py-slippi\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import slippi as slp\n",
        "\n",
        "!pip install -U -q PyDrive\n",
        "import os\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from google.colab import drive\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "gdrive = GoogleDrive(gauth)\n",
        "\n",
        "\n",
        "local_download_path = os.path.expanduser('~/data')\n",
        "try:\n",
        "  os.makedirs(local_download_path)\n",
        "except: pass\n",
        "\n",
        "drive.mount('/content/drive/')\n",
        "\n",
        "file_list = gdrive.ListFile(\n",
        "    {'q': \"'1Nqg7wW2bV5d7vzj1YbasIaLkByn5WZYI' in parents\"}).GetList() \n",
        "\n",
        "match_names = [file_list[i]['title'] for i in range(len(file_list))]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oyXn-tmFLYgT"
      },
      "source": [
        "##BUILDING THE MASTER DATAFRAME FROM 25,000 matches"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ycj39o-HLVvX"
      },
      "source": [
        "def get_characters_played(match):\n",
        "  test_str = str(match.start.players)\n",
        "  substart = 'CSSCharacter.'\n",
        "  subend = 'costume='\n",
        "  #find the indexes of the strings immediately before and after the character names\n",
        "  #then adjust the indexes to just give us the char names.\n",
        "  start = [i.start() for i in re.finditer(substart, test_str)]\n",
        "  start[0] += 13\n",
        "  start[1] += 13\n",
        "  end  = [i.start() for i in re.finditer(subend, test_str)]\n",
        "  end[0] += -2\n",
        "  end[1] += -2\n",
        "  char1 = test_str[start[0]: end[0]]\n",
        "  char2 = test_str[start[1]: end[1]]\n",
        "  return char1, char2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QX-bfLUPLg1H"
      },
      "source": [
        "#this takes 4 hours\n",
        "\n",
        "active_ports = []\n",
        "list_of_stages = []\n",
        "list_of_characters = []\n",
        "list_of_match_lengths = list([])\n",
        "\n",
        "for index in range(len(match_names)):\n",
        "  current_match_object = slp.Game('/content/drive/My Drive/slippiDB/' + match_names[index])\n",
        "  list_of_characters += [get_characters_played(current_match_object)]\n",
        "  print(get_characters_played(current_match_object))\n",
        "  list_of_stages.append(str(current_match_object.start.stage)[6:])\n",
        "  print(str(current_match_object.start.stage)[6:])\n",
        "  active_ports.append([current_match_object.start.players[i] is not None for i in range(4)])\n",
        "  print(index)\n",
        "  list_of_match_lengths.append(len(current_match_object.frames))\n",
        "  print(len(current_match_object.frames))\n",
        "  del current_match_object\n",
        "\n",
        "df = pd.DataFrame()\n",
        "df['stage'] = pd.Series(list_of_stages)\n",
        "df['characters'] = pd.Series(list_of_characters)\n",
        "df['file_names'] = pd.Series(match_names)\n",
        "df['length_in_frames'] = pd.Series(list_of_match_lengths)\n",
        "df['active_ports'] = pd.Series(active_ports)\n",
        "\n",
        "used_ports = [] #manual mapping for ease grabbing x,y from frames\n",
        "for i in range(len(df)):\n",
        "  if df.active_ports[i] == [True, True, False, False]:\n",
        "    used_ports += [[0,1]]\n",
        "  if df.active_ports[i] == [True, False, True, False]:\n",
        "    used_ports += [[0,2]]\n",
        "  if df.active_ports[i] == [True, False, False, True]:\n",
        "    used_ports += [[0,3]]\n",
        "  if df.active_ports[i] == [False, True, True, False]:\n",
        "    used_ports += [[1,2]]\n",
        "  if df.active_ports[i] == [False, True, False, True]:\n",
        "    used_ports += [[1,3]]\n",
        "  if df.active_ports[i] == [False, False, True, True]:\n",
        "    used_ports += [[2,3]]\n",
        "\n",
        "df['active_ports'] = pd.Series(used_ports)\n",
        "\n",
        "df.to_csv('slippiDB.csv')\n",
        "!cp slippiDB.csv \"drive/My Drive/\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7mqsFQ_PMAZr"
      },
      "source": [
        "##FINDING DESIRED SUBSET OF DATA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DRnclREsnP2C"
      },
      "source": [
        "link to slippiDB.csv https://drive.google.com/open?id=1VR7ucC5Tf31uoXlJsb4cWWaevD7y4Tzw"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P9Dn9rSPL2Rw"
      },
      "source": [
        "#You fell asleep and your runtime disconnected\n",
        "df = pd.read_csv('drive/My Drive/slippiDB.csv', index_col=0)\n",
        "\n",
        "from ast import literal_eval\n",
        "for index in range(len(df)):\n",
        "  df.active_ports[index] = literal_eval(df.active_ports[index])\n",
        "  df.characters[index] = literal_eval(df.characters[index])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ZgxnJYpMVqX"
      },
      "source": [
        "#make two conditionals to sort down to only fox vs fox on battlefield\n",
        "is_fox_ditto = np.array([df.characters[i][0] == 'FOX' and df.characters[i][1] == 'FOX' for i in range(len(df))])\n",
        "is_battlefield = np.array([df.stage[i] == 'BATTLEFIELD' for i in range(len(df))])\n",
        "data = df[is_fox_ditto & is_battlefield] #data is the new working dataframe"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zj5yY1wpMp1n"
      },
      "source": [
        "#look at ultimate size of dataset referenced by data dataframe, should be about 4 million frames, useful for calculating when you're going to be waiting\n",
        "\n",
        "sum_of_frames = 0\n",
        "\n",
        "for i in data.index:\n",
        "  sum_of_frames += data['length_in_frames'][i]\n",
        "\n",
        "sum_of_frames "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "41BmfpfWOaSw"
      },
      "source": [
        "##BUILD X_FEATURE MATRIX AND Y_TARGET VECTOR"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kKUWhGM3M5zf"
      },
      "source": [
        "#functions we'll need to wrangle that data!\n",
        "def get_ports(match):\n",
        "  used_ports = []\n",
        "  active_ports = [match.start.players[i] is not None for i in range(4)]\n",
        "  if active_ports == [True, True, False, False]:\n",
        "    used_ports = [0,1]\n",
        "  if active_ports == [True, False, True, False]:\n",
        "   used_ports = [0,2]\n",
        "  if active_ports == [True, False, False, True]:\n",
        "    used_ports = [0,3]\n",
        "  if active_ports == [False, True, True, False]:\n",
        "    used_ports = [1,2]\n",
        "  if active_ports == [False, True, False, True]:\n",
        "    used_ports = [1,3]\n",
        "  if active_ports == [False, False, True, True]:\n",
        "    used_ports = [2,3]\n",
        "  return used_ports\n",
        "\n",
        "def get_buttons(logical):\n",
        "  logical_string = str(logical)\n",
        "  inputs =      {'START':0,\n",
        "                 'Y':0,\n",
        "                 'X':0,                \n",
        "                 'B':0,\n",
        "                 'A':0,\n",
        "                 'L':0,\n",
        "                 'R':0,\n",
        "                 'Z':0,\n",
        "                 'NONE':0,\n",
        "                }\n",
        "  if logical_string.find('NONE') > -1:\n",
        "    inputs['NONE'] = 1\n",
        "    return inputs\n",
        "  if logical_string.find('|A') > -1 or logical_string.find('.A') > -1:\n",
        "    inputs['A'] = 1\n",
        "  if logical_string.find('|B') > -1 or logical_string.find('.B') > -1:\n",
        "    inputs['B'] = 1\n",
        "  if logical_string.find('|Y') > -1 or logical_string.find('.Y') > -1:\n",
        "    inputs['Y'] = 1\n",
        "  if logical_string.find('|X') > -1 or logical_string.find('.X') > -1:\n",
        "    inputs['X'] = 1\n",
        "  if logical_string.find('|L') > -1 or logical_string.find('.L') > -1:\n",
        "    inputs['L'] = 1\n",
        "  if logical_string.find('|R') > -1 or logical_string.find('.R') > -1:\n",
        "    inputs['R'] = 1\n",
        "  if logical_string.find('|Z') > -1 or logical_string.find('.Z') > -1:\n",
        "    inputs['Z'] = 1\n",
        "  #if logical_string.find('CSTICK_RIGHT') > -1:\n",
        "  #  inputs['CSTICK_RIGHT'] = 1\n",
        "  #if logical_string.find('CSTICK_LEFT') > -1:\n",
        "  #  inputs['CSTICK_LEFT'] = 1\n",
        "  #if logical_string.find('CSTICK_DOWN') > -1:\n",
        "  #  inputs['CSTICK_DOWN'] = 1\n",
        "  #if logical_string.find('CSTICK_UP') > -1:\n",
        "  #  inputs['CSTICK_UP'] = 1\n",
        "  #if logical_string.find('JOYSTICK_RIGHT') > -1:\n",
        "  #  inputs['JOYSTICK_RIGHT'] = 1\n",
        "  #if logical_string.find('JOYSTICK_LEFT') > -1:\n",
        "  #  inputs['JOYSTICK_LEFT'] = 1\n",
        "  #if logical_string.find('JOYSTICK_DOWN') > -1:\n",
        "  #  inputs['JOYSTICK_DOWN'] = 1\n",
        "  #if logical_string.find('JOYSTICK_UP') > -1:\n",
        "  #  inputs['JOYSTICK_UP'] = 1\n",
        "  return  inputs\n",
        "\n",
        "def to_y_vector_df(match_number, frame_number, frame, portslist):\n",
        "\n",
        "  ydf = pd.DataFrame()\n",
        "  ydf['frame'] = pd.Series([frame_number-1])\n",
        "  ydf['match'] = pd.Series([match_number])\n",
        "\n",
        "  p2_post = frame.ports[portslist[1]].leader.post\n",
        "  p2_state = p2_post.state.name\n",
        "  p2_state_age = int(np.round(p2_post.state_age))\n",
        "  p2_post_state = str(p2_state + ':' + str(p2_state_age))\n",
        " \n",
        "  ydf['p2_post_state'] = pd.Series([p2_post_state])\n",
        "\n",
        "  return ydf\n",
        "\n",
        "def to_x_matrix_df(match_number, frame_number, frame, portslist):\n",
        "\n",
        "  xdf = pd.DataFrame()\n",
        "  xdf['frame'] = pd.Series([frame_number])\n",
        "  xdf['match'] = pd.Series([match_number])\n",
        "\n",
        "  p1_pre = frame.ports[portslist[0]].leader.pre\n",
        "  p2_pre = frame.ports[portslist[1]].leader.pre\n",
        "  p1_post = frame.ports[portslist[0]].leader.post\n",
        "  p2_post = frame.ports[portslist[1]].leader.post\n",
        "\n",
        "  p1_buttons = get_buttons(p1_pre.buttons.logical)\n",
        "  xdf['p1_A'] = pd.Series([p1_buttons['A']])\n",
        "  xdf['p1_B'] = pd.Series([p1_buttons['B']])\n",
        "  xdf['p1_X'] = p1_buttons['X']\n",
        "  xdf['p1_Y'] = p1_buttons['Y']\n",
        "  xdf['p1_L'] = p1_buttons['L']\n",
        "  xdf['p1_R'] = p1_buttons['R']\n",
        "\n",
        "\n",
        "  #get p1 pre message\n",
        "  xdf['p1_pre_direction'] = p1_pre.direction.value\n",
        "  xdf['p1_cstick_x'] = p1_pre.cstick.x\n",
        "  xdf['p1_cstick_y'] = p1_pre.cstick.y\n",
        "  xdf['p1_jstick_x'] = p1_pre.joystick.x\n",
        "  xdf['p1_jstick_y'] = p1_pre.joystick.y\n",
        "  xdf['p1_L_trigger'] = p1_pre.triggers.physical.l\n",
        "  xdf['p1_R_trigger'] = p1_pre.triggers.physical.r\n",
        "  xdf['p1_pre_damage'] = p1_pre.damage[0]\n",
        "  xdf['p1_x_pos'] = p1_pre.position.x\n",
        "  xdf['p1_y_pos'] = p1_pre.position.y\n",
        "  xdf['p1_pre_state'] = p1_pre.state.name\n",
        "\n",
        "  p2_buttons = get_buttons(p2_pre.buttons.logical)\n",
        "  xdf['p2_A'] = p2_buttons['A']\n",
        "  xdf['p2_B'] = p2_buttons['B']\n",
        "  xdf['p2_X'] = p2_buttons['X']\n",
        "  xdf['p2_Y'] = p2_buttons['Y']\n",
        "  xdf['p2_L'] = p2_buttons['L']\n",
        "  xdf['p2_R'] = p2_buttons['R']\n",
        "\n",
        "\n",
        "  #get p2 pre message\n",
        "  xdf['p2_pre_direction'] = p2_pre.direction.value\n",
        "  xdf['p2_cstick_x'] = p2_pre.cstick.x\n",
        "  xdf['p2_cstick_y'] = p2_pre.cstick.y\n",
        "  xdf['p2_jstick_x'] = p2_pre.joystick.x\n",
        "  xdf['p2_jstick_y'] = p2_pre.joystick.y\n",
        "  xdf['p2_L_trigger'] = p2_pre.triggers.physical.l\n",
        "  xdf['p2_R_trigger'] = p2_pre.triggers.physical.r\n",
        "  xdf['p2_pre_damage'] = p2_pre.damage[0]\n",
        "  xdf['p2_pre_x_pos'] = p2_pre.position.x\n",
        "  xdf['p2_pre_y_pos'] = p2_pre.position.y\n",
        "  xdf['p2_pre_state'] = p2_pre.state.name\n",
        "\n",
        "  #get p1 post message\n",
        "  xdf['p1_is_airborne'] = p1_post.airborne\n",
        "  xdf['p1_post_damage'] = p1_post.damage\n",
        "  xdf['p1_post_direction'] = p1_post.direction.value\n",
        "  xdf['p1_hitstun'] = int(np.floor(p1_post.hit_stun))\n",
        "  xdf['p1_jumps_remaining'] = p1_post.jumps\n",
        "  if p1_post.l_cancel is not None:\n",
        "    if p1_post.l_cancel.value is 2:\n",
        "      xdf['p1_Lcancel_failed'] = 1\n",
        "    else:\n",
        "      xdf['p1_Lcancel_failed'] = 0\n",
        "  else:\n",
        "    xdf['p1_Lcancel_failed'] = 0\n",
        "  xdf['p1_post_x_pos'] = p1_post.position.x\n",
        "  xdf['p1_post_y_pos'] = p1_post.position.y\n",
        "  xdf['p1_shield'] = p1_post.shield\n",
        "  p1_state = p1_post.state.name\n",
        "  p1_state_age = int(np.round(p1_post.state_age))\n",
        "  p1_post_state = str(p1_state + ':' + str(p1_state_age))\n",
        "  xdf['p1_post_state'] = p1_post_state\n",
        "  xdf['p1_stocks'] = p1_post.stocks\n",
        "\n",
        "  #get p2 post message\n",
        "  xdf['p2_is_airborne'] = p2_post.airborne\n",
        "  xdf['p2_post_damage'] = p2_post.damage\n",
        "  xdf['p2_post_direction'] = p2_post.direction.value\n",
        "  xdf['p2_hitstun'] = int(np.floor(p2_post.hit_stun))\n",
        "  xdf['p2_jumps_remaining'] = p2_post.jumps\n",
        "  if p2_post.l_cancel is not None:\n",
        "    if p2_post.l_cancel.value is 2:\n",
        "      xdf['p2_Lcancel_failed'] = 1\n",
        "    else:\n",
        "      xdf['p2_Lcancel_failed'] = 0\n",
        "  else:\n",
        "    xdf['p2_Lcancel_failed'] = 0\n",
        "  xdf['p2_post_x_pos'] = p2_post.position.x\n",
        "  xdf['p2_post_y_pos'] = p2_post.position.y\n",
        "  xdf['p2_shield'] = p2_post.shield\n",
        "  p2_state = p2_post.state.name\n",
        "  p2_state_age = int(np.round(p2_post.state_age))\n",
        "  p2_post_state = str(p2_state + ':' + str(p2_state_age))\n",
        "  xdf['p2_post_state'] = p2_post_state\n",
        "  xdf['p2_stocks'] = p2_post.stocks\n",
        "\n",
        "  return xdf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c0V6FhI0NHRA"
      },
      "source": [
        "frames = []\n",
        "target_frames = []\n",
        "\n",
        "for index in range(0,len(data)): \n",
        "  #using your generated data dataframe, go through, load each match, and pull out all the features I think are useful for the matrix\n",
        "  #also grab the y_target vector\n",
        "  current_match = slp.Game('/content/drive/My Drive/slippiDB/' + data['file_names'][data.index[index]]) #this assumes you have the slippi database on a google drive\n",
        "  \n",
        "  for i in range(1, len(current_match.frames), 2):\n",
        "    newframe = to_y_vector_df(index, i, current_match.frames[i], get_ports(current_match))\n",
        "    print(index, i)\n",
        "    target_frames += [newframe]\n",
        "\n",
        "  for i in range(0, len(current_match.frames)-1, 2):\n",
        "    newframe = to_x_matrix_df(index, i, current_match.frames[i], get_ports(current_match))\n",
        "    print(index, i)\n",
        "    frames += [newframe]\n",
        "\n",
        "x_feature_matrix = pd.concat(frames) #TAKES DOUBLE FOREVER\n",
        "y_target_vector = pd.concat(target_frames) #TAKES FOREVER\n",
        "x_feature_matrix.set_index(['match','frame'], inplace=True)\n",
        "y_target_vector.set_index(['match','frame'], inplace=True)\n",
        "\n",
        "x_feature_matrix.to_csv('X_matrix.csv')\n",
        "!cp X_matrix.csv \"drive/My Drive/\"\n",
        "\n",
        "y_target_vector.to_csv('Y_vector.csv')\n",
        "!cp Y_vector.csv \"drive/My Drive/\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VZkguLWLXkMW"
      },
      "source": [
        "URLs for my generated files since github wont let you upload anything over 25MB\n",
        "\n",
        "https://drive.google.com/open?id=1W5QjBp1llr2znj1bG2CgFKQcuKoOqbOo #X_Matrix\n",
        "\n",
        "https://drive.google.com/open?id=1LTosyLQzCe_cxL8bJ3LfBwDvrwM4Dfx7 #Y_Vector"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z_fcZ8T8OyXr"
      },
      "source": [
        "#It's been 30 hours, your colab session has expired\n",
        "Xdata = pd.read_csv('drive/My Drive/X_matrix.csv', index_col=[0, 1])\n",
        "ydata  = pd.read_csv('drive/My Drive/Y_target.csv', index_col=[0, 1])\n",
        "\n",
        "#sync up the stratified matrix and vector\n",
        "Xdata = Xdata.reset_index()\n",
        "ydata = ydata.reset_index()\n",
        "Xdata = Xdata.drop('match', axis=1)\n",
        "ydata = ydata.drop(['match', 'frame'], axis=1)\n",
        "Xdata['p2_next_frame_post_state'] = ydata['p2_post_state'] \n",
        "\n",
        "#split into your train and val out of the testing data, your testing data is huge\n",
        "from sklearn.model_selection import train_test_split\n",
        "test, train = train_test_split(Xdata, test_size=0.05, random_state=19)\n",
        "test, val = train_test_split(test, test_size=.1, random_state=19)\n",
        "print(len(train), len(val))\n",
        "\n",
        "#get the data ready to be fed into models\n",
        "target = 'p2_next_frame_post_state'\n",
        "X_train = train.drop(columns=target)\n",
        "y_train = train[target]\n",
        "X_val = val.drop(columns=target)\n",
        "y_val = val[target]\n",
        "#best stay away from test, it's too big\n",
        "X_test = test.drop(columns=target)\n",
        "y_test = test[target]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dKTePcnlQIY0"
      },
      "source": [
        "##TRAIN MODEL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d5wiboxAP0bA"
      },
      "source": [
        "import category_encoders as ce\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.pipeline import make_pipeline\n",
        "import numpy as np\n",
        "\n",
        "pipeline = make_pipeline(\n",
        "    ce.OrdinalEncoder(), \n",
        "    DecisionTreeClassifier()\n",
        ")\n",
        "\n",
        "pipeline.fit(X_train, y_train)\n",
        "\n",
        "print('Accuracy:', pipeline.score(X_val, y_val)) #it might run out of RAM here especially if train was split > .3, maybe put this in another cell"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EpelzBw-QHY3"
      },
      "source": [
        "#save the results in case you crash and lose everything\n",
        "y_pred = pipeline.predict(X_val)\n",
        "model_results = pd.DataFrame()\n",
        "model_results['predicted'] = pd.Series(y_pred)\n",
        "model_results['true'] = pd.Series(y_val.values)\n",
        "model_results.to_csv('results.csv')\n",
        "!cp results.csv \"drive/My Drive/model results/\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qTr6dmOOmMnn"
      },
      "source": [
        "since I'm recompiling this colab from A LOT of different colabs I don't have the model itself, though you can reproduce it. I did save the results of DecisionTree-Basic on .05X of the data https://drive.google.com/open?id=1G_cOw6bJNl_0u1adthTluqB5GfVBhBgc if you load this as a dataframe it has predicted and True columns for about 60,000 frames"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pQo5L9A6Q--r"
      },
      "source": [
        "##GET FEATURE IMPORTANCE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VkNwEr4TQqio"
      },
      "source": [
        "#assuming you still have the model loaded\n",
        "\n",
        "import eli5\n",
        "from eli5.sklearn import PermutationImportance\n",
        "\n",
        "dt = DecisionTreeClassifier()\n",
        "encoder = ce.OrdinalEncoder()\n",
        "\n",
        "X_train_transformed = encoder.fit_transform(X_train)\n",
        "X_val_transformed = encoder.transform(X_val)\n",
        "\n",
        "dt.fit(X_train_transformed, y_train)\n",
        "\n",
        "permuter = PermutationImportance(\n",
        "    dt, \n",
        "    scoring='accuracy', \n",
        "    n_iter=5, #might run out of RAM, you can reduce\n",
        "    random_state=21\n",
        ")\n",
        "\n",
        "permuter.fit(X_val_transformed, y_val)\n",
        "\n",
        "feature_names = X_test.columns.tolist()\n",
        "\n",
        "importances = pd.Series(permuter.feature_importances_, feature_names).sort_values()\n",
        "\n",
        "eli5.show_weights(\n",
        "    permuter, \n",
        "    top=None, \n",
        "    feature_names=feature_names\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ak59SDpVSESx"
      },
      "source": [
        "##FEATURE ENGINEERING\n",
        "sorry I lost my code for adding distance and angle to the X_matrix, but it's very simple loop of going to each frame, pulling the p1 and p2 x,y coordinates, and doing pythagorean theorem and then finding the angle, but here is the voronoi generator assuming you have the data df loaded from above. Also I added the code for grabbing the frame in the match in above already"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-DEgRInIScvn"
      },
      "source": [
        "#make empty lists to be filled with coordinates\n",
        "fox_pos = []\n",
        "\n",
        "pos = {'FOX':fox_pos #just so we can use keys to pass around the lists of coordinates}\n",
        "\n",
        "#GET ALL COORDINATE PAIRS\n",
        "\n",
        "for index in range(len(df)):\n",
        "  current_game_object = slp.Game('/content/drive/My Drive/slippi DB/' + data.file_names[index])\n",
        "  print(index, ' :opening file: ', data.file_names[index])\n",
        "  print(data.characters[index][0], '...getting frames')\n",
        "  pos[data.characters[index][0]] += [current_game_object.frames[frame].ports[data.active_ports[index][0]].leader.post.position for frame in range(data.length_in_frames[index])] #put p1s coordinates in correct pos list by key\n",
        "  print(data.characters[index][1], '...getting frames\\n')\n",
        "  pos[data.characters[index][1]] += [current_game_object.frames[frame].ports[data.active_ports[index][1]].leader.post.position for frame in range(data.length_in_frames[index])] #put p2s corrdinates in correct pos list by key"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L-EwsGSmTSCo"
      },
      "source": [
        "all_points_df = pd.DataFrame()\n",
        "all_points_df['x'] = pd.Series([all_points[i].x for i in range(len(all_points))])\n",
        "all_points_df['y'] = pd.Series([all_points[i].y for i in range(len(all_points))])\n",
        "all_points_df.to_csv('ALL_POINTS.csv')\n",
        "!cp ALL_POINTS.csv \"drive/My Drive/\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "chHzJ14tTU3A"
      },
      "source": [
        "kmeans = KMeans(n_clusters=12)\n",
        "clusters = kmeans.fit_predict(all_points_df)\n",
        "centers = kmeans.cluster_centers_\n",
        "Xdata['P2_Zone'] = pd.Series(clusters)\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}