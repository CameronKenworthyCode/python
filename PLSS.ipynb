{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PLSS.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOq4WooaA9Qjb5BxGzjz1lY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CameronKenworthyCode/python/blob/main/PLSS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Built T-R-S map data by querying BLM public land survey**"
      ],
      "metadata": {
        "id": "w4nwX0DPb_3g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Setup"
      ],
      "metadata": {
        "id": "WJt-oknvbMlE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 572
        },
        "id": "hnZoPPAqY6tA",
        "outputId": "a0600913-b3af-4d67-c011-4426e2d7e2e0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pymongo[srv] in /usr/local/lib/python3.7/dist-packages (4.1.1)\n",
            "Requirement already satisfied: dnspython<3.0.0,>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from pymongo[srv]) (2.2.1)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: dnspython in /usr/local/lib/python3.7/dist-packages (2.2.1)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (2.23.0)\n",
            "Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1\n",
            "  Using cached urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests) (2022.6.15)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests) (3.0.4)\n",
            "Installing collected packages: urllib3\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.26.10\n",
            "    Uninstalling urllib3-1.26.10:\n",
            "      Successfully uninstalled urllib3-1.26.10\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "selenium 4.3.0 requires urllib3[secure,socks]~=1.26, but you have urllib3 1.25.11 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Successfully installed urllib3-1.25.11\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "urllib3"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: bs4 in /usr/local/lib/python3.7/dist-packages (0.0.1)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/dist-packages (from bs4) (4.6.3)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: html2text in /usr/local/lib/python3.7/dist-packages (2020.1.16)\n",
            "Mounted at /content/drive/\n"
          ]
        }
      ],
      "source": [
        "!pip install requests\n",
        "!pip install bs4\n",
        "!pip install html2text\n",
        "!pip install -U -q PyDrive\n",
        "\n",
        "import html2text\n",
        "from ast import literal_eval\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from google.colab import drive\n",
        "from oauth2client.client import GoogleCredentials\n",
        "import random\n",
        "import json\n",
        "\n",
        "auth.authenticate_user() #establish connection to a gdrive to write the eventual csv file to\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "gdrive = GoogleDrive(gauth)\n",
        "\n",
        "local_download_path = os.path.expanduser('~/data')\n",
        "try:\n",
        "  os.makedirs(local_download_path)\n",
        "except: pass\n",
        "\n",
        "drive.mount('/content/drive/', force_remount=True)\n",
        "\n",
        "file_list = gdrive.ListFile(\n",
        "    {'q': \"'1DWFETkLt-dWcA7CfymejJe7O-UJxSR4B' in parents and trashed=False\"}).GetList() "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TODO\n",
        "\n",
        "~~- generate URLS~~\n",
        "\n",
        "~~- batch query~~\n",
        "~~- save geometry long/lat coords~~\n",
        "\n",
        "  ~~- number of coords given varies per section~~\n",
        "    \n",
        "    - get corners\n",
        "      - get all points along each edge\n",
        "    - infer outline of township range from edges of sections\n",
        "- infer shape from coords\n",
        "- save all as csv\n",
        "- code to calculate if coord is in shape"
      ],
      "metadata": {
        "id": "w3hZ6xzYac9C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Generate URL Queries"
      ],
      "metadata": {
        "id": "Kbieh-fuclFA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "T/R pairs in Washington State:\n",
        "\n",
        "TOWN: 1N -> 41N\n",
        "\n",
        "RANGE: 16W -> 47E\n"
      ],
      "metadata": {
        "id": "0ARVMQCLd4jk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# generate set of strings in form WA330##0N0##0E0SN##0 or WA330##0N0##0W0SN##0\n",
        "# return list of these strings of possible sections 1-60 for each T/R pair\n",
        "def generate_url(town, _range):\n",
        "  t_ = town[:-1]\n",
        "  if int(t_) < 10:\n",
        "    t_ = '0' + t_\n",
        "  r_ = _range[:-1]\n",
        "  if int(r_) < 10:\n",
        "    r_ = '0' + r_\n",
        "  r_ = r_ + '0' + _range[-1:]\n",
        "  base_string = 'WA330'+ t_ + '0N0' + r_ + '0SN'\n",
        "  section_list = []\n",
        "  for i in range(1, 60):\n",
        "    s_ = str(i) + '0'\n",
        "    if i < 10:\n",
        "      s_ = '0' + str(i) + '0'\n",
        "    section_list.append(base_string+s_)\n",
        "  \n",
        "  return section_list"
      ],
      "metadata": {
        "id": "ksa7mr0kncZR"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# generate urls in batches of 25 queries\n",
        "trs = []\n",
        "\n",
        "# for all T/R where R is West\n",
        "for i in range(1,42):\n",
        "  for j in range(1,17):\n",
        "    trs.append([str(i)+'N', str(j)+'W'])\n",
        "\n",
        "# for all T/R where R is East\n",
        "for i in range(1, 42):\n",
        "  for j in range(1, 48):\n",
        "    trs.append([str(i)+'N', str(j)+'E'])\n",
        "\n",
        "# All the section url words for each T/R\n",
        "url_dict = {}\n",
        "for pair in trs:\n",
        "  key = pair[0] + '-' + pair[1]\n",
        "  value = generate_url(pair[0], pair[1])\n",
        "  url_dict[key] = value\n",
        "\n",
        "#strings to build url query\n",
        "base_url = 'https://gis.blm.gov/arcgis/rest/services/Cadastral/BLM_Natl_PLSS_CadNSDI/MapServer/exts/CadastralSpecialServices/GetLatLon?trs='\n",
        "built_url = ''\n",
        "base_url_end = '&returnalllevels=&f=pjson'\n",
        "\n",
        "# go through each T/R pair, and take the next 25 section urls to build a query, save the results and continue to the next 25\n",
        "count = 0\n",
        "keys = {} # for building PLSS_dict\n",
        "urls = [] # for sending batches to requests.get()\n",
        "for i in range(len(trs)):\n",
        "  # for each T/R pair\n",
        "  for j in range(0, 59):\n",
        "    # for each possible section in each T/R pair\n",
        "    if count == 25: # url query maxed, store and refresh\n",
        "      urls.append(base_url+built_url+base_url_end)\n",
        "      built_url = ''\n",
        "      count = 0\n",
        "    count += 1\n",
        "    built_url = built_url + url_dict[trs[i][0] + '-' + trs[i][1]][j] + '+%7C+'\n",
        "    # create dict of WA330##0N0##0E0SN##0 keys corresponding to regular T/R/S notation\n",
        "    # use dict to correctly put away coordinates to correct T/R/S in PLSS_dict\n",
        "    keys[url_dict[trs[i][0] + '-' + trs[i][1]][j]] = trs[i][0] + '-' + trs[i][1] + '-' + str(j+1) "
      ],
      "metadata": {
        "id": "yelOJsT9114Y"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Query gis.blm.gov for T/R/S coordinate shapes"
      ],
      "metadata": {
        "id": "YeObJTDrbDgm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# fill dict with coordinate outlines of each section in every T/R\n",
        "# by querying gis.blm.gov\n",
        "\n",
        "PLSS_dict_raw = {}\n",
        "\n",
        "for i in range(len(urls)):\n",
        "  url = urls[i]\n",
        "  parser = html2text.HTML2Text()\n",
        "  parser.ignore_links = True\n",
        "  result = requests.get(url)\n",
        "  src = result.content\n",
        "  soup = BeautifulSoup(src, 'lxml')\n",
        "  result = json.loads(soup.text)\n",
        "  print(str(round(i/len(urls)*100, 3)) + ': ' + str(len(result['features'])))\n",
        "  for i in range(len(result['features'])):\n",
        "    #{'TT-RR-SS': [[List of section defining coordinates]]}\n",
        "    PLSS_dict_raw[keys[result['features'][i]['attributes']['landdescription']]] = result['features'][i]['geometry']['rings'][0]\n",
        "\n",
        "plss_df = pd.DataFrame({ key:pd.Series(value) for key, value in PLSS_dict_raw.items() })\n",
        "plss_df = plss_df.T\n",
        "plss_df.to_csv('plss.csv')\n",
        "!cp plss.csv \"drive/My Drive/NEWICC/PLSS/\""
      ],
      "metadata": {
        "id": "sZvvEmFKNnfK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}