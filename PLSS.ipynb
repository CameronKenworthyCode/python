{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PLSS.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPFrBu2IQfr77UJlkBvRMiA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CameronKenworthyCode/python/blob/main/PLSS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Built T-R-S map data by querying BLM public land survey**"
      ],
      "metadata": {
        "id": "w4nwX0DPb_3g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Setup"
      ],
      "metadata": {
        "id": "WJt-oknvbMlE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hnZoPPAqY6tA",
        "outputId": "9ed8c764-3104-4897-f0dc-8dabeadf4841"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (2.23.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests) (2022.6.15)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests) (1.25.11)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: bs4 in /usr/local/lib/python3.7/dist-packages (0.0.1)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/dist-packages (from bs4) (4.6.3)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: html2text in /usr/local/lib/python3.7/dist-packages (2020.1.16)\n",
            "Mounted at /content/drive/\n"
          ]
        }
      ],
      "source": [
        "!pip install requests\n",
        "!pip install bs4\n",
        "!pip install html2text\n",
        "!pip install -U -q PyDrive\n",
        "\n",
        "import html2text\n",
        "from ast import literal_eval\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from google.colab import drive\n",
        "from oauth2client.client import GoogleCredentials\n",
        "import random\n",
        "import json\n",
        "\n",
        "auth.authenticate_user() #establish connection to a gdrive to write the eventual csv file to\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "gdrive = GoogleDrive(gauth)\n",
        "\n",
        "local_download_path = os.path.expanduser('~/data')\n",
        "try:\n",
        "  os.makedirs(local_download_path)\n",
        "except: pass\n",
        "\n",
        "drive.mount('/content/drive/', force_remount=True)\n",
        "\n",
        "file_list = gdrive.ListFile(\n",
        "    {'q': \"'1DWFETkLt-dWcA7CfymejJe7O-UJxSR4B' in parents and trashed=False\"}).GetList() \n",
        "\n",
        "EWA_TRS = pd.read_csv('drive/My Drive/NEWICC/PLSS/EWA_trs.csv', index_col=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TODO\n",
        "\n",
        "~~- generate URLS~~\n",
        "\n",
        "~~- batch query~~\n",
        "~~- save geometry long/lat coords~~\n",
        "\n",
        "  ~~- number of coords given varies per section~~\n",
        "    \n",
        "    - get corners\n",
        "      - get all points along each edge\n",
        "    - infer outline of township range from edges of sections\n",
        "- infer shape from coords\n",
        "- save all as csv\n",
        "- code to calculate if coord is in shape"
      ],
      "metadata": {
        "id": "w3hZ6xzYac9C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Functions"
      ],
      "metadata": {
        "id": "XpAlARcd1wFc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# generate set of strings in form WA330##0N0##0E0SN##0 or WA330##0N0##0W0SN##0\n",
        "# return list of these strings of possible sections 1-60 for each T/R pair\n",
        "def generate_url(town, _range):\n",
        "  t_ = town[:-1]\n",
        "  if int(t_) < 10:\n",
        "    t_ = '0' + t_\n",
        "  r_ = _range[:-1]\n",
        "  if int(r_) < 10:\n",
        "    r_ = '0' + r_\n",
        "  r_ = r_ + '0' + _range[-1:]\n",
        "  base_string = 'WA330'+ t_ + '0N0' + r_ + '0SN'\n",
        "  section_list = []\n",
        "  for i in range(1, 60):\n",
        "    s_ = str(i) + '0'\n",
        "    if i < 10:\n",
        "      s_ = '0' + str(i) + '0'\n",
        "    section_list.append(base_string+s_)\n",
        "  \n",
        "  return section_list"
      ],
      "metadata": {
        "id": "ksa7mr0kncZR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Generate URL Queries"
      ],
      "metadata": {
        "id": "Kbieh-fuclFA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "T/R pairs in Washington State:\n",
        "\n",
        "TOWN: 1N -> 41N\n",
        "\n",
        "RANGE: 16W -> 47E\n"
      ],
      "metadata": {
        "id": "0ARVMQCLd4jk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# generate urls in batches of 25 queries\n",
        "trs = []\n",
        "\n",
        "# for all T/R where R is West\n",
        "for i in range(1,42):\n",
        "  for j in range(1,17):\n",
        "    trs.append([str(i)+'N', str(j)+'W'])\n",
        "\n",
        "# for all T/R where R is East\n",
        "for i in range(1, 42):\n",
        "  for j in range(1, 48):\n",
        "    trs.append([str(i)+'N', str(j)+'E'])\n",
        "\n",
        "# All the section url words for each T/R\n",
        "url_dict = {}\n",
        "for pair in trs:\n",
        "  key = pair[0] + '-' + pair[1]\n",
        "  value = generate_url(pair[0], pair[1])\n",
        "  url_dict[key] = value\n",
        "\n",
        "#strings to build url query\n",
        "base_url = 'https://gis.blm.gov/arcgis/rest/services/Cadastral/BLM_Natl_PLSS_CadNSDI/MapServer/exts/CadastralSpecialServices/GetLatLon?trs='\n",
        "built_url = ''\n",
        "base_url_end = '&returnalllevels=&f=pjson'\n",
        "\n",
        "# go through each T/R pair, and take the next 25 section urls to build a query, save the results and continue to the next 25\n",
        "count = 0\n",
        "keys = {} # for building PLSS_dict\n",
        "urls = [] # for sending batches to requests.get()\n",
        "for i in range(len(trs)):\n",
        "  # for each T/R pair\n",
        "  for j in range(0, 59):\n",
        "    # for each possible section in each T/R pair\n",
        "    if count == 25: # url query maxed, store and refresh\n",
        "      urls.append(base_url+built_url+base_url_end)\n",
        "      built_url = ''\n",
        "      count = 0\n",
        "    count += 1\n",
        "    built_url = built_url + url_dict[trs[i][0] + '-' + trs[i][1]][j] + '+%7C+'\n",
        "    # create dict of WA330##0N0##0E0SN##0 keys corresponding to regular T/R/S notation\n",
        "    # use dict to correctly put away coordinates to correct T/R/S in PLSS_dict\n",
        "    keys[url_dict[trs[i][0] + '-' + trs[i][1]][j]] = trs[i][0] + '-' + trs[i][1] + '-' + str(j+1) "
      ],
      "metadata": {
        "id": "yelOJsT9114Y"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Query gis.blm.gov for T/R/S coordinate shapes"
      ],
      "metadata": {
        "id": "YeObJTDrbDgm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "\n",
        "ALREADY RAN\n",
        "TAKES A LONG TIME\n",
        "JUST LOAD CSV INSTEAD\n",
        "\n",
        "'''\n",
        "# fill dict with coordinate outlines of each section in every T/R\n",
        "# by querying gis.blm.gov\n",
        "\n",
        "PLSS_dict_raw = {}\n",
        "\n",
        "for i in range(len(urls)):\n",
        "  url = urls[i]\n",
        "  parser = html2text.HTML2Text()\n",
        "  parser.ignore_links = True\n",
        "  result = requests.get(url)\n",
        "  src = result.content\n",
        "  soup = BeautifulSoup(src, 'lxml')\n",
        "  result = json.loads(soup.text)\n",
        "  print(str(round(i/len(urls)*100, 3)) + ': ' + str(len(result['features'])))\n",
        "  for i in range(len(result['features'])):\n",
        "    #{'TT-RR-SS': [[List of section defining coordinates]]}\n",
        "    PLSS_dict_raw[keys[result['features'][i]['attributes']['landdescription']]] = result['features'][i]['geometry']['rings'][0]\n",
        "\n",
        "plss_df = pd.DataFrame({ key:pd.Series(value) for key, value in PLSS_dict_raw.items() })\n",
        "plss_df = plss_df.T\n",
        "plss_df.to_csv('plss.csv')\n",
        "!cp plss.csv \"drive/My Drive/NEWICC/PLSS/\"\n",
        "\n",
        "# lookup table for easy search\n",
        "tr_index = {}\n",
        "\n",
        "# for each valid T/R/S word\n",
        "# create a row of their integer T/R/S values\n",
        "for index in plss_df.index:\n",
        "  trs_list = index.split('-')\n",
        "  if trs_list[1][-1:] == 'W':\n",
        "    continue\n",
        "  town = int(trs_list[0][:-1])\n",
        "  range = int(trs_list[1][:-1])\n",
        "  section = int(trs_list[2])\n",
        "  tr_index[index] = {\"town\": town, 'range':range, 'section':section}\n",
        "\n",
        "# build final dataframe\n",
        "tr_index_df = pd.DataFrame(tr_index).T\n",
        "trs_df = tr_index_df.join(plss_df)\n",
        "trs_df.to_csv('EWA_trs.csv')\n",
        "!cp EWA_trs.csv \"drive/My Drive/NEWICC/PLSS/\""
      ],
      "metadata": {
        "id": "sZvvEmFKNnfK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}